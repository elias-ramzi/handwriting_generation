{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/elias/work/descript-research-test\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "from data import DataSynthesis\n",
    "from utils import plot_stroke, json_default\n",
    "from models.handwriting_synthesis import HandWritingSynthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID = int(time.time())\n",
    "\n",
    "LOAD_PREVIOUS = None\n",
    "\n",
    "MODEL_PATH = 'models/trained/test/model_synthesis_{}.h5'.format(RUN_ID)\n",
    "EPOCH_MODEL_PATH = 'models/trained/test/model_synthesis_{}_{}.h5'.format(RUN_ID, '{}')\n",
    "DATA_PATH = 'data/strokes-py3.npy'\n",
    "HISTORY_PATH = 'models/history/test/history_{}.json'.format(RUN_ID)\n",
    "LOG_PATH = 'models/logs/'\n",
    "\n",
    "VERBOSE = False\n",
    "\n",
    "model_kwargs = {\n",
    "    'lr': .0001,\n",
    "    'rho': .95,\n",
    "    'momentum': .9,\n",
    "    'epsilon': .0001,\n",
    "    'centered': True,\n",
    "}\n",
    "\n",
    "HIDDEN_DIM = 400\n",
    "NUM_LAYERS = 3\n",
    "\n",
    "data_kwargs = {\n",
    "    'path_to_data': DATA_PATH,\n",
    "    'train_split': 0.9\n",
    "}\n",
    "\n",
    "train_generator_kwargs = {\n",
    "    'batch_size': 1,\n",
    "    'shuffle': False,\n",
    "}\n",
    "\n",
    "validation_generator_kwargs = {\n",
    "    'batch_size': 1,\n",
    "    'shuffle': True,\n",
    "}\n",
    "\n",
    "EPOCHS = 1\n",
    "STEPS_PER_EPOCH = 1\n",
    "VAL_STEPS = 1\n",
    "MODEL_CHECKPOINT = 5\n",
    "\n",
    "# bias for writing ~~style~~\n",
    "BIAS = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = DataSynthesis(**data_kwargs)\n",
    "WINDOW_SIZE = len(D.sentences[0][0])\n",
    "\n",
    "model_kwargs['vocab_size'] = WINDOW_SIZE\n",
    "hws = HandWritingSynthesis(**model_kwargs)\n",
    "hws.make_model()\n",
    "tensorboard_cb = TensorBoard(log_dir=LOG_PATH)\n",
    "tensorboard_cb.set_model(hws.model)\n",
    "\n",
    "nan = False\n",
    "generator = D.batch_generator(\n",
    "    **train_generator_kwargs,\n",
    ")\n",
    "validation_generator = D.batch_generator(\n",
    "    **validation_generator_kwargs,\n",
    ")\n",
    "\n",
    "# XXX: use the get_initial_state of WindowedLSTMCell\n",
    "input_states = [\n",
    "    # stateh1, statec1\n",
    "    tf.zeros((1, HIDDEN_DIM), dtype=float), tf.zeros((1, HIDDEN_DIM), dtype=float),\n",
    "    # stateh2, statec2\n",
    "    tf.zeros((1, HIDDEN_DIM), dtype=float), tf.zeros((1, HIDDEN_DIM), dtype=float),\n",
    "    # stateh3, statec3\n",
    "    tf.zeros((1, HIDDEN_DIM), dtype=float), tf.zeros((1, HIDDEN_DIM), dtype=float),\n",
    "    # window kappa\n",
    "    tf.zeros((1, WINDOW_SIZE), dtype=float), tf.zeros((1, 10), dtype=float),\n",
    "    # phi, alpha, beta\n",
    "    tf.zeros((1, 1), dtype=float), tf.zeros((1, 10), dtype=float), tf.zeros((1, 10), dtype=float),\n",
    "]\n",
    "\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'validation_loss': [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for overfitting\n",
    "strokes, sentence, targets = next(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(1, EPOCHS + 1):\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for s in tqdm(range(1, STEPS_PER_EPOCH+1), desc=\"Epoch {}/{}\".format(e, EPOCHS)):\n",
    "        # strokes, sentence, targets = next(generator)\n",
    "        loss = hws.train(strokes, sentence, input_states, targets)\n",
    "        train_loss.append(loss)\n",
    "\n",
    "        if loss is np.nan:\n",
    "            nan = True\n",
    "            print('exiting train @epoch : {}'.format(e))\n",
    "            break\n",
    "\n",
    "    for _ in range(VAL_STEPS):\n",
    "        vstrokes, vsentence, vtargets = next(validation_generator)\n",
    "        val_loss.append(hws.validation(vstrokes, vsentence, input_states, vtargets))\n",
    "\n",
    "    mean_loss = np.mean(train_loss)\n",
    "    mean_val_loss = np.mean(val_loss)\n",
    "    history['train_loss'].append(mean_loss)\n",
    "    history['validation_loss'].append(mean_val_loss)\n",
    "    print(\"Epoch {:03d}: Loss: {:.3f} / Validation loss : {:.3f}\"\n",
    "          .format(e, mean_loss, mean_val_loss))\n",
    "\n",
    "    if e % MODEL_CHECKPOINT == 0:\n",
    "        hws.model.save_weights(EPOCH_MODEL_PATH.format(e))\n",
    "\n",
    "    if nan:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not nan:\n",
    "    hws.save_weights(MODEL_PATH)\n",
    "    \n",
    "with open(HISTORY_PATH, 'w') as f:\n",
    "    json.dump(history, f, default=json_default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose_sentence = \"\".join(D.encoder.inverse_transform(sentence)[0])\n",
    "strokes1, windows, phis, kappas, alphas, betas = hws.infer(\n",
    "    sentence, inf_type='max',\n",
    "    verbose=verbose_sentence,\n",
    ")\n",
    "weights = np.stack([np.squeeze(x.numpy()) for x in phis], axis=1)\n",
    "target = tf.gather(targets, [2, 0, 1], axis=2)[0].numpy()\n",
    "strokes1 = D.scale_back(strokes1)\n",
    "target = D.scale_back(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Learning curv')\n",
    "plt.plot(history['train_loss'], label='Training learn curv')\n",
    "plt.plot(history['validation_loss'], color='r', label='Validation learn curv')\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Weights over steps')\n",
    "plt.imshow(weights, cmap='plasma')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stroke(strokes1)\n",
    "plot_stroke(target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
